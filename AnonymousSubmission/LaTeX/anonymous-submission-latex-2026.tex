%File: anonymous-submission-latex-2026.tex
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage[submission]{aaai2026}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\usepackage{amsmath}  % For mathematical typesetting
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in} % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in} % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}

%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2026.1)
}

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai2026.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\title{Video Comprehension Score (VCS): A Metric for Long-Form Video Description Evaluation}
\author{
    %Authors
    % All authors must be in the same font size and format.
    Written by AAAI Press Staff\textsuperscript{\rm 1}\thanks{With help from the AAAI Publications Committee.}\\
    AAAI Style Contributions by Pater Patel Schneider,
    Sunil Issar,\\
    J. Scott Penberthy,
    George Ferguson,
    Hans Guesgen,
    Francisco Cruz\equalcontrib,
    Marc Pujol-Gonzalez\equalcontrib
}
\affiliations{
    %Afiliations
    \textsuperscript{\rm 1}Association for the Advancement of Artificial Intelligence\\
    % If you have multiple authors and multiple affiliations
    % use superscripts in text and roman font to identify them.
    % For example,

    % Sunil Issar\textsuperscript{\rm 2},
    % J. Scott Penberthy\textsuperscript{\rm 3},
    % George Ferguson\textsuperscript{\rm 4},
    % Hans Guesgen\textsuperscript{\rm 5}
    % Note that the comma should be placed after the superscript

    1101 Pennsylvania Ave, NW Suite 300\\
    Washington, DC 20004 USA\\
    % email address must be in roman text type, not monospace or sans serif
    proceedings-questions@aaai.org
%
% See more examples next
}

%Example, Single Author, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\iffalse
\title{My Publication Title --- Single Author}
\author {
    Author Name
}
\affiliations{
    Affiliation\\
    Affiliation Line 2\\
    name@example.com
}
\fi

\iffalse
%Example, Multiple Authors, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\title{My Publication Title --- Multiple Authors}
\author {
    % Authors
    First Author Name\textsuperscript{\rm 1},
    Second Author Name\textsuperscript{\rm 2},
    Third Author Name\textsuperscript{\rm 1}
}
\affiliations {
    % Affiliations
    \textsuperscript{\rm 1}Affiliation 1\\
    \textsuperscript{\rm 2}Affiliation 2\\
    firstAuthor@affiliation1.com, secondAuthor@affilation2.com, thirdAuthor@affiliation1.com
}
\fi


% REMOVE THIS: bibentry
% This is only needed to show inline citations in the guidelines document. You should not need it and can safely delete it.
\usepackage{bibentry}
% END REMOVE bibentry

\begin{document}

\maketitle

\begin{figure*}[t]
\centering
\includegraphics[width=0.8\textwidth]{VCS}
\caption{Architecture of the Video Comprehension Score (VCS). The VCS assesses video descriptions by comparing a reference text ($T_{ref}$) with a model-generated text ($T_{gen}$). Both texts are initially segmented by SaT and embedded via nvEmbed. The Global Alignment Score (GAS) is computed from the full text embeddings. For localized analysis, texts are chunked and embedded, forming a similarity matrix. From this, precision and recall-oriented best matches yield the Local Alignment Score (LAS)—the harmonic mean of $LAS_P$ (precision) and $LAS_R$ (recall). The Narrative Alignment Score (NAS) incorporates distance-based ($NAS_D$) and line-based ($NAS_L$) assessments. $NAS_D$ and $NAS_L$ are harmonic means of their respective precision and recall components. A Window Regularizer ($R_w$) refines the NAS. The Semantic Alignment Score (SAS) is derived by modulating GAS with LAS. The final VCS results from modulating the smaller of SAS and the regularized NAS with the larger.}
\label{fig:vcs-architecture}
\end{figure*}

\begin{abstract}
Evaluating long-form video descriptions is challenging, as traditional metrics like BLEU, ROUGE, and METEOR inadequately capture narrative coherence, temporal ordering, and semantic equivalence beyond lexical overlap. We introduce Video Comprehension Score (VCS), a comprehensive metric assessing semantic alignment, temporal accuracy, and corruption sensitivity. VCS integrates three components: Global Alignment Score for global thematic similarity, Local Alignment Score for local semantic correspondence, and Narrative Alignment Score for temporal coherence with adjustable tolerance. VCS is validated on two large-scale synthetic test sets derived from 1,390 scene-level descriptions from the MPII Movie Description dataset. For the Comparison Test Set, these scenes are aggregated into baseline descriptions ($\approx$500 words each) via ChatGPT-4o, then augmented with 10 valid and 10 invalid stylistic or corrupt variants, totaling 27,800 descriptions. For the Multiple-Author Test Set, the same 1,390 scene-level descriptions are aggregated using four different LLMs, resulting in 5,560 descriptions representing realistic authorial variation. VCS consistently outperforms traditional metrics, being the only metric capable of distinguishing valid from invalid variants in the Comparison Test Set. On the Multiple-Author Test Set, it achieves superior cross-author consistency, being the only metric with $>$80\% accuracy across four language model references. VCS$_{\text{short}}$, our implementation of VCS for short captions, attains state-of-the-art human correlation on VATEX-EVAL in the 9-ref setting (Kendall's $\tau=41.5$, Spearman's $\rho=52.8$) and competitive results in the 1-ref setting (Kendall's $\tau=30.0$, Spearman's $\rho=38.1$). These results demonstrate the effectiveness of VCS for robustly assessing video description quality across diverse narrative styles and lengths.
\end{abstract}

% Uncomment the following to link to your code, datasets, an extended version or similar.
% You must keep this block between (not within) the abstract and the main body of the paper.
% \begin{links}
%     \link{Code}{https://aaai.org/example/code}
%     \link{Datasets}{https://aaai.org/example/datasets}
%     \link{Extended version}{https://aaai.org/example/extended-version}
% \end{links}

\section{Introduction}

Recent advancements in Large Video Language Models (LVLMs) \citep{Yuan2025Tarsier2,Shen2025LongVU,Ataallah2024Goldfish,Chen2025LongVILA} have substantially enhanced automated video comprehension, enabling extensive and detailed descriptions from complex content. This advancement is critical for applications requiring nuanced interpretation, such as autonomous navigation and assistive technologies. However, accurately evaluating genuine comprehension—rather than frame-level element identification—remains challenging, particularly with long videos. Existing evaluation frameworks primarily utilize question-answering or caption-based methods \citep{wu2024longvideobench,ataallah2024infinibench,mangalam2023egoschema,zhou2025mlvu}. Question-answering effectively probes specific details but inadequately measures holistic comprehension. Caption-based methods better capture overall narrative comprehension but fail to effectively assess specific detail recognition. Given our primary concern with comprehensive video comprehension, caption-based evaluations are more suitable.

Traditional caption-based evaluation techniques include n-gram metrics such as BLEU \citep{p:02}, METEOR \citep{bl:05}, CIDEr \citep{v:15}, and ROUGE \citep{l:04}, embedding-based metrics such as BERTScore \citep{z:20}, and contemporary LLM-based approaches such as CLAIR \citep{chan:23} and AutoDQ \citep{wyzs:24}, each exhibiting critical shortcomings with extensive LVLM-generated descriptions. N-gram metrics rely on lexical overlap, unfairly penalizing legitimate stylistic variations, increasingly problematic as description length grows. Embedding-based metrics address lexical overlap by focusing on semantic similarity but remain constrained by limited context lengths, less effective for very long descriptions, and inadequately evaluate chronological accuracy. LLM-driven methods provide deeper semantic insights but depend heavily on the inherent accuracy of the underlying LLM and often lack consistency.

Evaluating long-form descriptions introduces two principal challenges:
\begin{enumerate}
\item \textbf{Expressive Variability}: Extended descriptions allow multiple valid articulations differing in detail, style, and length. Conversely, invalid expressions involving omissions or distortions may occur. An effective evaluation metric must discriminate valid variations without unnecessary penalties and detect invalid articulations accurately.
\item \textbf{Narrative Variability}: Extended descriptions may validly represent events in different chronological sequences or present genuinely incorrect sequences. A robust evaluation method must distinguish valid variations without unjust penalties while identifying incorrect sequences accurately.
\end{enumerate}

This paper introduces the Video Comprehension Score (VCS), a novel metric addressing these complexities by evaluating dense, long-form descriptions semantically and structurally. VCS employs Segment Any Text (SaT) \citep{frohmann-etal-2024-segment} for semantic segmentation and NvEmbed-v2 \citep{l:24} for paragraph and chunk-level embeddings, comprising three components:
\begin{enumerate}
\item \textbf{Global Alignment Score (GAS)}: Measures overall thematic similarity using full-text embeddings.
\item \textbf{Local Alignment Score (LAS)}: Assesses fine-grained semantic correspondence between chunks.
\item \textbf{Narrative Alignment Score (NAS)}: Evaluates chronological consistency using a configurable local chronology tolerance factor (LCT) to control the strictness of chronological evaluation.
\end{enumerate}

Combining GAS and LAS yields the Semantic Alignment Score (SAS), representing semantic alignment across long paragraphs. Integrating SAS with NAS produces the comprehensive VCS metric, clearly assessing narrative equivalence between model-generated and human-written descriptions. Additionally, we generalize VCS to VCS$_{\text{short}}$, applying the same principles to shorter captions.

Due to the absence of suitable annotated datasets for dense, long-form descriptions—where human judgment becomes increasingly unreliable—we constructed a large-scale synthetic dataset (1390 descriptions, $\sim$500 words each, from the MPII Movie Description dataset \citep{rohrbach2015dataset} via ChatGPT4o). Two test sets, a Comparison Test Set (27,800 description pairs) and a Multiple-Author Test Set (5560 variants from four LLMs), comprehensively evaluate VCS performance. Benchmarked against traditional metrics (BLEU, METEOR, ROUGE and etc.), VCS consistently demonstrated insensitivity to valid variations and sensitivity to invalid variations as compared to traditional metrics. On VATEX-EVAL, VCS$_{\text{short}}$ achieved state-of-the-art results in the 9-reference setting and a close second in the 1-reference setting. Results affirm VCS as a reliable metric accurately assessing narrative equivalence across diverse descriptive styles and lengths, configurable for varying chronological rigor.

Our primary contributions include:
\begin{itemize}
\item A robust metric (VCS) for both long-form and short-form video descriptions.
\item Configurable LCT and chunk size parameters to govern chronological strictness and comparison granularity.
\end{itemize}

\section{Related Work}

Traditional n-gram-based metrics such as BLEU \citep{p:02}, ROUGE \citep{l:04}, and METEOR \citep{bl:05} evaluate text generation through lexical overlap and local word order. BLEU measures n-gram precision with brevity penalties, capturing local chronology but struggles with expressive variability. ROUGE variants emphasize recall; ROUGE-N evaluates n-gram overlap, while ROUGE-L utilizes the Longest Common Subsequence (LCS) at sentence-level, yet remains sensitive to expressive variability and sentence-length discrepancies. METEOR addresses expressive variability via synonyms and stems, computing precision-recall harmonics with fragmentation penalties for local word order. CIDEr \citep{v:15} addresses expressive variability by consensus-based TF-IDF weighting across multiple references but proves impractical for long-form descriptions due to labor-intensive annotation. SPICE \citep{afjg:16} evaluates semantic propositions using graph overlaps, effectively handling paraphrasing; however, being designed for static images, it cannot be directly applied to videos and neglects the fluency, grammar, and narrative chronology critical for video descriptions.

Embedding-based metrics compare texts in semantic vector spaces, leveraging pretrained models to capture semantic similarity beyond lexical matches. Early methods like BERTScore \citep{z:20}, MoverScore \citep{z:19}, and SBERT \citep{r:19} address expressive variability by recognizing paraphrases but are constrained by limited context windows, complicating their direct application to long-form descriptions. Recent decoder-based models such as nv-embed-v2 \citep{l:24}, Linq-Embed-Mistral \citep{cklg:24}, SFR-Embedding-Mistral \citep{mljx:24}, and Jasper and Stella \citep{zlw:24} offer significantly larger context windows and robust global embeddings, excelling at paragraph-level semantic assessments. However, reliance on global embeddings and cosine similarity overlooks local content alignment, detailed information accuracy, and chronological coherence, allowing subtle inaccuracies or misordered events to remain undetected.

Multimodal embedding metrics like EMScore \citep{syxl:22} and PAC-S \citep{sbc:23} employ vision-language models such as CLIP \citep{Radford2021LearningTV} to evaluate semantic alignment between visuals and generated captions, addressing expressive variability through direct image-caption comparison, thus bypassing reference caption. EMScore combines coarse and fine-grained multimodal matches for accurate short-caption evaluation, whereas PAC-S, fine-tuned via contrastive learning, closely aligns with human judgments. Despite their effectiveness in short-form descriptions, these metrics face computational challenges and methodological limitations when scaling to long-form descriptions.

Recent evaluation approaches increasingly leverage Large Language Models (LLMs), categorized into component-based and holistic judge methods. Component-based methods such as AutoDQ \citep{wyzs:24} and VAD-Score \citep{dp:25} use LLMs for semantic extraction and entailment checks, effectively addressing the expressive variability challenge; however, AutoDQ and VAD do not evaluate the chronology of events. Nonetheless, their effectiveness depends heavily on extraction accuracy, consistency across model updates, scalability with dense content, and reliance on comprehensive references. Conversely, holistic methods such as CapScore \citep{li:24}, CapArena‑Auto Score \citep{cheng:25}, and CLAIR \citep{chan:23} provide overall quality assessments directly from LLMs, theoretically addressing complex evaluation dimensions comprehensively. However, they suffer from ambiguity in score calibration, sensitivity to prompting nuances, consistency issues across model versions, limited interpretability, and practical constraints, including reproducibility and cost.

\section{Methodology: Video Comprehension Score (VCS)}
\label{sec:methodology_vcs}

Figure~\ref{fig:VCS} illustrates the VCS architecture, a comprehensive metric designed to evaluate long-form video descriptions through semantic and narrative alignment assessment. VCS employs NV-Embed~\cite{lee2024nv} to extract embeddings at both global and fine-grained levels, enabling quantitative similarity measurement across different granularities of text analysis.

\paragraph{Global Alignment Score (GAS)}
VCS begins with global semantic assessment to capture overall thematic similarity between reference and generated texts. Entire texts $T_{ref}$ and $T_{gen}$ are embedded via NV-Embed to yield global vectors $E_{ref}$ and $E_{gen}$. GAS computes their cosine similarity:
\begin{equation} \label{eq:gas_revised}
\text{GAS} = \cos(E_{ref}, E_{gen}) = \frac{E_{ref} \cdot E_{gen}}{\|E_{ref}\| \|E_{gen}\|}
\end{equation}

While GAS effectively measures thematic congruence, it cannot capture local semantic agreement or chronological ordering, necessitating fine-grained chunk-level analysis.

\paragraph{Text Preprocessing for Chunk-Level Analysis}
To enable fine-grained comparison, VCS establishes correspondences between smaller text units. The preprocessing pipeline first cleans input texts $T_{ref}$ and $T_{gen}$ by removing punctuation, then applies Segment any Text (SaT)~\cite{frohmann2024segment} segmentation to produce semantic segments $S_{ref}$ and $S_{gen}$. To balance granularity with contextual coherence, $k$ consecutive segments form ``chunks,'' yielding sequences $C_{ref}$ and $C_{gen}$. These chunks are embedded using NV-Embed into matrices $E_{C_{ref}} \in \mathbb{R}^{N_{ref} \times D}$ and $E_{C_{gen}} \in \mathbb{R}^{N_{gen} \times D}$, enabling quantitative similarity measurement essential for Local and Narrative Alignment Scores.

\paragraph{Defining Mapping Windows}
Before establishing chunk correspondences, VCS defines Mapping Windows (MW) that constrain the search space for optimal alignments. This concept stems from empirical observations in text similarity matrices: when computing pairwise embeddings between identical stories using sentence transformers, the resulting similarity matrix exhibits a clear diagonal structure. The first sentence from one text has highest cosine similarity with the first sentence from the other, and this diagonal pattern holds even when stories have different lengths—in brevity cases (shorter generated text) or verbosity cases (longer generated text), the diagonal pattern stretches or compresses proportionally.

Mapping windows formalize this observation by defining permissible regions where text chunks can match if stories align in narrative structure. Based on element counts $N_{ref}, N_{gen}$, length ratio $r = \max(N_{ref}, N_{gen}) / \min(N_{ref}, N_{gen})$, and base window height $h_{mw} = \lceil r \rceil$, the system calculates Precision Windows ($MW_{prec}$) and Recall Windows ($MW_{rec}$) that accommodate natural variations in description length and detail.

\paragraph{Best Matching Algorithm}
When comparing texts chunk-by-chunk, naive highest-similarity pairing creates semantic ambiguity and collisions. Chunks often exhibit high similarity with multiple candidates—for instance, both generated chunks might score 0.80 with the same reference chunk (collision), or a generated chunk might superficially match due to lexical overlap despite another reference chunk being its true narrative counterpart (ambiguity). The Best Matching Algorithm resolves these issues by using adaptive context windows and mapping window constraints to find optimal one-to-one alignments that respect narrative structure.

For each source element, the algorithm identifies maximum similarity $M_j$ and, if exceeding a context cutoff (0.6), defines an adaptive similarity margin creating a candidate band. Among candidates within this band, it selects the element closest to its expected narrative position within the mapping window, breaking ties by highest raw similarity. This bidirectional process yields precision-oriented ($M_P$) and recall-oriented ($M_R$) best matches that maintain semantic quality while respecting chronological constraints.

\paragraph{Local Alignment Score (LAS)}
LAS assesses fine-grained semantic quality by averaging cosine similarities of matched chunk pairs. It computes precision-oriented $LAS_P = \frac{\sum_{(c_{j}^{gen},c_{m(j)}^{ref})\in M_P}\text{Sim}(c_{j}^{gen},c_{m(j)}^{ref})}{|M_P|}$ and recall-oriented $LAS_R = \frac{\sum_{(c_{i}^{ref},c_{m(i)}^{gen})\in M_R}\text{Sim}(c_{i}^{ref},c_{m(i)}^{gen})}{|M_R|}$, then combines them via harmonic mean:

\begin{equation} \label{eq:las_revised}
LAS = F_1(LAS_P, LAS_R) =
\begin{cases}
\frac{2 \cdot LAS_P \cdot LAS_R}{LAS_P + LAS_R} & \text{if } LAS_P + LAS_R > 0 \\
0 & \text{otherwise}
\end{cases}
\end{equation}

High LAS indicates strong local semantic agreement, though it remains insensitive to chronological order, necessitating narrative assessment.

\paragraph{Narrative Alignment Score (NAS)}
NAS evaluates chronological coherence and narrative integrity using the matched pairs $M_P$ and $M_R$ established by the Best Matching Algorithm. It addresses LAS's order insensitivity through two complementary approaches: distance-based penalties for positional deviations and line-based path analysis for narrative flow assessment.

\paragraph{Local Chronology Tolerance (LCT)}
Before computing NAS components, VCS defines Local Chronology Tolerance through multiplier $\tau_{LCT} \geq 0$ that provides configurable flexibility for benign local reorderings and minor content variations. When $\tau_{LCT}=0$, strict chronological order is enforced. Higher values allow increasingly relaxed chronological constraints, enabling the metric to accommodate natural variations in storytelling while maintaining narrative coherence assessment.

\paragraph{Distance-based NAS ($NAS_D$)}
This component penalizes when matched elements from $M_P$ and $M_R$ deviate from their expected narrative positions within Mapping Windows. For each matched pair, the algorithm computes raw positional distance from the mapping window boundaries, then applies LCT tolerance to determine effective distance $d'$. Matches within mapping windows or within LCT tolerance receive zero penalty, while violations contribute to total penalty $P_{total,x}$, normalized by maximum possible penalty $P_{max,x}$:

\begin{equation} \label{eq:nas_dx_revised}
NAS_{D,x} =
\begin{cases}
1 - \frac{P_{total,x}}{P_{max,x}} & \text{if } P_{max,x} > 0 \\
1.0 & \text{if } P_{max,x} = 0 \text{ and } P_{total,x} = 0 \\
0.0 & \text{if } P_{max,x} = 0 \text{ and } P_{total,x} > 0
\end{cases}
\end{equation}
where $x \in \{P, R\}$.

\paragraph{Line-based NAS ($NAS_L$)}
This component analyzes the geometric path formed by consecutive matched elements from $M_P$ and $M_R$, treating alignment as a trajectory through the similarity matrix. Using dynamic programming, VCS computes ideal narrative bands bounded by shortest ($L_{floor,ideal,x}$) and longest ($L_{ceil,ideal,x}$) possible paths through the mapping windows. The actual path length ($L_{actual,x}$) between consecutive matches incorporates LCT-based segment filtering, where segments exceeding LCT thresholds are either adjusted or excluded. Paths within ideal bands score perfectly, while deviations are penalized proportionally:

\begin{equation} \label{eq:nas_lx_revised}
NAS_{L,x} =
\begin{cases}
1.0 & \text{if } L_{floor,ideal,x} \leq L_{actual,x} \leq L_{ceil,ideal,x} \\
L_{actual,x} / L_{floor,ideal,x} & \text{if } L_{actual,x} < L_{floor,ideal,x} \text{ and } L_{floor,ideal,x} > 0 \\
L_{ceil,ideal,x} / L_{actual,x} & \text{if } L_{actual,x} > L_{ceil,ideal,x} \text{ and } L_{actual,x} > 0 \\
0.0 & \text{otherwise}
\end{cases}
\end{equation}
where $x \in \{P, R\}$.

The final NAS integrates both approaches: $NAS_{F1} = F_1(NAS_D, NAS_L)$, where $NAS_D = F_1(NAS_{D,P}, NAS_{D,R})$ and $NAS_L = F_1(NAS_{L,P}, NAS_{L,R})$.

\paragraph{Window Regularization}
To mitigate influence from extreme text length disparities, a Window Regularizer ($R_W$) adjusts $NAS_{F1}$. Using mapping window area $A_{total,mw}$, timeline area $A_{timeline}$, and minimum ratio $A_{min,ratio} = 1/N_{max}$:

\begin{equation}
R_W = 
\begin{cases}
\max\left(0, \min\left(1, \frac{(A_{total,mw} / A_{timeline}) - A_{min,ratio}}{0.5 - A_{min,ratio}}\right)\right) & \text{if } A_{timeline} > 0 \text{ and } A_{min,ratio} < 1 \\
0 & \text{otherwise}
\end{cases}
\end{equation}

The regularized score $NAS_{reg} = \frac{NAS_{F1} - R_W}{1 - R_W}$ (when $NAS_{F1} - R_W > 0$ and $R_W < 1$) ensures meaningful chronological assessment even with significant length variations.

\paragraph{Final VCS Aggregation}
The complete VCS integrates semantic and narrative alignment through careful score combination. GAS is modulated by LAS to yield Semantic Alignment Score ($SAS$):
\begin{equation} \label{eq:sas_revised} 
SAS = 
\begin{cases}
\frac{GAS - (1 - LAS)}{LAS} & \text{if } LAS > 0 \text{ and } (GAS - (1 - LAS)) > 0 \\
0.0 & \text{otherwise}
\end{cases}
\end{equation}

This modulation ensures that high global similarity requires supporting local semantic agreement. The final score synthesizes $SAS$ with $NAS_{reg}$ through adaptive weighting:

\begin{align}
S_{num} &= 
\begin{cases}
SAS - (1 - NAS_{reg}) & \text{if } SAS < NAS_{reg} \\
NAS_{reg} - (1 - SAS) & \text{otherwise}
\end{cases} \\
S_{den} &= 
\begin{cases}
NAS_{reg} & \text{if } SAS < NAS_{reg} \\
SAS & \text{otherwise}
\end{cases}
\end{align}

\begin{equation} \label{eq:vcs_revised}
VCS =
\begin{cases}
\frac{S_{num}}{S_{den}} & \text{if } S_{num} > 0 \text{ and } S_{den} \neq 0 \\
0.0 & \text{otherwise}
\end{cases}
\end{equation}

\paragraph{Extension for Short Captions (VCS$_{short}$)}
For short caption evaluation, VCS$_{short}$ adapts the complete VCS methodology to operate at word-level granularity. Input texts undergo cleaning to remove punctuation and stop words, then tokenization into individual words that serve as fundamental elements. These words replace multi-word chunks in all alignment and scoring processes while maintaining identical metric computation and aggregation logic, enabling consistent evaluation across different description lengths.

% TODO: Add additional sections like Experiments, Results, Discussion, Conclusion here

% For now, adding placeholder for remaining sections
\section{Experiments}
% Add your experiments section here

\section{Results}
% Add your results section here

\section{Conclusion}
% Add your conclusion section here

\section*{Acknowledgments}
% Add acknowledgments here if needed

\bibliography{aaai2026}

\end{document}
