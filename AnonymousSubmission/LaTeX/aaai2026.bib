% Video Language Models
@misc{Yuan2025Tarsier2,
  author       = "Liping Yuan and Jiawei Wang and Haomiao Sun and Yuchen Zhang and Yuan Lin",
  title        = "{Tarsier2: Advancing Large Vision-Language Models from Detailed Video Description to Comprehensive Video Understanding}",
  year         = 2025,
  eprint       = "2501.07888",
  archivePrefix= "arXiv",
  primaryClass = "cs.CV"
}

@inproceedings{Shen2025LongVU,
  author    = "Xiaoqian Shen and Yunyang Xiong and Changsheng Zhao and Lemeng Wu and Jun Chen and Chenchen Zhu and Zechun Liu and Fanyi Xiao and Balakrishnan Varadarajan and Florian Bordes and Zhuang Liu and Hu Xu and Hyunwoo J. Kim and Bilge Soran and Raghuraman Krishnamoorthi and Mohamed Elhoseiny and Vikas Chandra",
  title     = "{LongVU: Spatiotemporal Adaptive Compression for Long Video-Language Understanding}",
  year      = 2025,
  booktitle = "Proceedings of the 42nd International Conference on Machine Learning {(ICML-25)}",
  address   = "Vancouver, Canada",
  publisher = "PMLR"
}

@inproceedings{Shu2025VideoXL,
  author    = "Yan Shu and Zheng Liu and Peitian Zhang and Minghao Qin and Junjie Zhou and Zhengyang Liang and Tiejun Huang and Bo Zhao",
  title     = "{Video-XL: Extra-Long Vision Language Model for Hour-Scale Video Understanding}",
  year      = 2025,
  booktitle = "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2025)",
  pages     = "26160-26169",
  address   = "Los Alamitos, Calif.",
  publisher = "IEEE"
}

@inproceedings{Chen2025LongVILA,
  author    = "Yukang Chen and Fuzhao Xue and Dacheng Li and Qinghao Hu and Ligeng Zhu and Xiuyu Li and Yunhao Fang and Haotian Tang and Shang Yang and Zhijian Liu and Yihui He and Hongxu Yin and Pavlo Molchanov and Jan Kautz and Linxi Fan and Yuke Zhu and Yao Lu and Song Han",
  title     = "{LongVILA: Scaling Long-Context Visual Language Models for Long Videos}",
  year      = 2025,
  booktitle = "Proceedings of the Thirteenth International Conference on Learning Representations {(ICLR-2025)}",
  address   = "Singapore",
  publisher = "ICLR"
}

@inproceedings{Ataallah2024Goldfish,
  author    = "Kirolos Ataallah and Xiaoqian Shen and Eslam Abdelrahman and Essam Sleiman and Mingchen Zhuge and Jian Ding and Deyao Zhu and J{\"u}rgen Schmidhuber and Mohamed Elhoseiny",
  title     = "{Goldfish: Vision-Language Understanding of Arbitrarily Long Videos}",
  year      = 2024,
  booktitle = "Proceedings of the European Conference on Computer Vision (ECCV 2024)",
  pages     = "251-267",
  address   = "Cham, Switzerland",
  publisher = "Springer"
}

@inproceedings{Weng2024LongVLM,
  author    = "Yuetian Weng and Mingfei Han and Haoyu He and Xiaojun Chang and Bohan Zhuang",
  title     = "{LongVLM: Efficient Long Video Understanding via Large Language Models}",
  year      = 2024,
  booktitle = "Proceedings of the European Conference on Computer Vision (ECCV 2024)",
  pages     = "453-470",
  address   = "Cham, Switzerland",
  publisher = "Springer"
}

@inproceedings{He2024MALMM,
  author    = "Bo He and Hengduo Li and Young Kyun Jang and Menglin Jia and Xuefei Cao and Ashish Shah and Abhinav Shrivastava and Ser{-}Nam Lim",
  title     = "{MA-LMM: Memory-Augmented Large Multimodal Model for Long-Term Video Understanding}",
  year      = 2024,
  booktitle = "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2024)",
  pages     = "13504-13514",
  address   = "Los Alamitos, Calif.",
  publisher = "IEEE"
}

@misc{Cheng2024LongVideoLLaMA,
  author       = "Zesen Cheng and Sicong Leng and Hang Zhang and Yifei Xin and Xin Li and Guanzheng Chen and Yongxin Zhu and Wenqi Zhang and Ziyang Luo and Deli Zhao and Lidong Bing and others",
  title        = "{LongVideo-LLaMA: Extending LLaMA for Long Video Understanding with Temporal Attention}",
  year         = 2024,
  note         = "arXiv preprint (under review)"
}

@inproceedings{Chen2024VideoLLMonline,
  author    = "Joya Chen and Zhaoyang Lv and Shiwei Wu and Qinghong (Kevin) Lin and Chenan Song and Difei Gao and Jia{-}Wei Liu and Ziteng Gao and Dongxing Mao and Mike Zheng Shou",
  title     = "{VideoLLM-online: Online Video Large Language Model for Streaming Video Understanding}",
  year      = 2024,
  booktitle = "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2024)",
  pages     = "18407-18418",
  address   = "Los Alamitos, Calif.",
  publisher = "IEEE"
}

@inproceedings{Ren2024TimeChat,
  author    = "Shuhuai Ren and Linli Yao and Shicheng Li and Xu Sun and Lu Hou",
  title     = "{TimeChat: A Time-sensitive Multimodal Large Language Model for Long Video Understanding}",
  year      = 2024,
  booktitle = "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2024)",
  pages     = "14313-14323",
  address   = "Los Alamitos, Calif.",
  publisher = "IEEE"
}

@inproceedings{Qian2024VideoStreaming,
  author    = "Rui Qian and Xiaoyi Dong and Pan Zhang and Yuhang Zang and Shuangrui Ding and Dahua Lin and Jiaqi Wang",
  title     = "{Streaming Long Video Understanding with Large Language Models}",
  year      = 2024,
  booktitle = "Proceedings of the 38th Conference on Neural Information Processing Systems (NeurIPS 2024)",
  address   = "Red Hook, NY",
  publisher = "Curran Associates"
}

% Video Understanding Benchmarks
@inproceedings{wu2024longvideobench,
  author = {Haoning Wu and Dongxu Li and Bei Chen and Junnan Li},
  year = 2024,
  title = "{LongVideoBench: A Benchmark for Long-context Interleaved Video-Language Understanding}",
  booktitle = "Advances in Neural Information Processing Systems 37 (NeurIPS-2024), Datasets and Benchmarks Track",
  address = "San Diego, CA",
  publisher = "Curran Associates"
}

@inproceedings{ataallah2024infinibench,
  author = {Kirolos Ataallah and Chenhui Gou and Eslam Abdelrahman and Khushbu Pahwa and Jian Ding and Mohamed Elhoseiny},
  year = 2025,
  title = "{InfiniBench: A Comprehensive Benchmark for Large Multimodal Models in Very Long Video Understanding}",
  booktitle = "Proceedings of the Thirteenth International Conference on Learning Representations (ICLR-2025)",
  address = "Singapore"
}

@misc{du2024eventoriented,
  author = {Yifan Du and Kun Zhou and Yuqi Huo and Yifan Li and Wayne Xin Zhao and Haoyu Lu and Zijia Zhao and Bingning Wang and Weipeng Chen and Ji-Rong Wen},
  year = 2024,
  title = "{Towards Event-oriented Long Video Understanding}",
  eprint = {2406.14129},
  archivePrefix = {arXiv},
  primaryClass = {cs.CV}
}

@inproceedings{fang2024mmbenchvideolongformmultishotbenchmark,
  author = {Xinyu Fang and Kangrui Mao and Haodong Duan and Xiangyu Zhao and Yining Li and Dahua Lin and Kai Chen},
  year = 2024,
  title = "{MMBench-Video: A Long-Form Multi-Shot Benchmark for Holistic Video Understanding}",
  booktitle = "Advances in Neural Information Processing Systems 37 (NeurIPS-2024), Datasets and Benchmarks Track",
  address = "San Diego, CA",
  publisher = "Curran Associates"
}

@inproceedings{nagrani2025neptune,
  author = {Arsha Nagrani and Mingda Zhang and Ramin Mehran and Rachel Hornung and Nitesh Bharadwaj Gundavarapu and Nilpa Jha and Austin Myers and Xingyi Zhou and Boqing Gong and Cordelia Schmid and Mikhail Sirotenko and Yukun Zhu and Tobias Weyand},
  year = 2025,
  title = "{Neptune: The Long Orbit to Benchmarking Long Video Understanding}",
  booktitle = "Proceedings of the Thirteenth International Conference on Learning Representations (ICLR-2025)",
  address = "Singapore"
}

@misc{wang2024lvbench,
  author = {Weihan Wang and Zehai He and Wenyi Hong and Yean Cheng and Xiaohan Zhang and Ji Qi and Xiaotao Gu and Shiyu Huang and Bin Xu and Yuxiao Dong and Ming Ding and Jie Tang},
  year = 2024,
  title = "{LVBench: An Extreme Long Video Understanding Benchmark}",
  eprint = {2406.08035},
  archivePrefix = {arXiv},
  primaryClass = {cs.CV}
}

@inproceedings{zhao2024videoniah,
  author = {Zijia Zhao and Haoyu Lu and Yuqi Huo and Yifan Du and Tongtian Yue and Longteng Guo and Bingning Wang and Weipeng Chen and Jing Liu},
  year = 2025,
  title = "{Needle in a Video Haystack: A Scalable Synthetic Framework for Benchmarking Video MLLMs}",
  booktitle = "Proceedings of the Thirteenth International Conference on Learning Representations (ICLR-2025)",
  address = "Singapore"
}

@inproceedings{zhou2025mlvu,
  author = {Junjie Zhou and Yan Shu and Bo Zhao and Boya Wu and Shitao Xiao and Xi Yang and Yongping Xiong and Bo Zhang and Tiejun Huang and Zheng Liu},
  year = 2025,
  title = "{MLVU: A Comprehensive Benchmark for Multi-Task Long Video Understanding}",
  booktitle = "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR-2025)",
  address = "Honolulu, HI",
  publisher = "IEEE"
}

@inproceedings{fu2025videomme,
  author = {Chaoyou Fu and Yuhan Dai and Yongdong Luo and Lei Li and Shuhuai Ren and Renrui Zhang and Zihan Wang and Chenyu Zhou and Yunhang Shen and Mengdan Zhang and Peixian Chen and Yanwei Li and Shaohui Lin and Sirui Zhao and Ke Li and Tong Xu and Xiawu Zheng and Enhong Chen and Caifeng Shan and Ran He and Xing Sun},
  year = 2025,
  title = "{Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis}",
  booktitle = "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR-2025)",
  address = "Honolulu, HI",
  publisher = "IEEE"
}

@inproceedings{mangalam2023egoschema,
  author = {Karttikeya Mangalam and Raiymbek Akshulakov and Jitendra Malik},
  year = 2023,
  title = "{EgoSchema: A Diagnostic Benchmark for Very Long-form Video Language Understanding}",
  booktitle = "Advances in Neural Information Processing Systems 36 (NeurIPS-2023), Datasets and Benchmarks Track",
  address = "New Orleans, LA",
  publisher = "Curran Associates"
}

@inproceedings{li2024mvbench,
  author = {Kunchang Li and Yali Wang and Yinan He and Yizhuo Li and Yi Wang and Yi Liu and Zun Wang and Limin Wang and Yu Qiao},
  year = 2024,
  title = "{MVBench: A Comprehensive Multi-modal Video Understanding Benchmark}",
  booktitle = "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR-2024)",
  pages = "22195--22206",
  publisher = "IEEE"
}

% Traditional Evaluation Metrics
@inproceedings{p:02,
  author = "Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing",
  year = 2002,
  title = "{BLEU: a Method for Automatic Evaluation of Machine Translation}",
  booktitle = "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
  pages = "311--318",
  address = "Philadelphia, Pa.",
  publisher = "Association for Computational Linguistics"
}

@inproceedings{l:04,
  author = "Lin, Chin-Yew",
  year = 2004,
  title = "{ROUGE: A Package for Automatic Evaluation of Summaries}",
  booktitle = "Text Summarization Branches Out: Proceedings of the ACL-04 Workshop",
  pages = "74--81",
  address = "Barcelona, Spain",
  publisher = "Association for Computational Linguistics"
}

@inproceedings{bl:05,
  author = "Banerjee, Satanjeev and Lavie, Alon",
  year = 2005,
  title = "{METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments}",
  booktitle = "Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization",
  pages = "65--72",
  address = "Ann Arbor, Mich.",
  publisher = "Association for Computational Linguistics"
}

@inproceedings{v:15,
  author = "Vedantam, Ramakrishna and Zitnick, C. Lawrence and Parikh, Devi",
  year = 2015,
  title = "{CIDEr: Consensus-Based Image Description Evaluation}",
  booktitle = "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2015)",
  pages = "4566--4575",
  address = "Washington, D.C.",
  publisher = "IEEE Computer Society"
}

@inproceedings{afjg:16,
  author = "Anderson, Peter and Fernando, Basura and Johnson, Mark and Gould, Stephen",
  year = 2016,
  title = "{SPICE: Semantic Propositional Image Caption Evaluation}",
  booktitle = "Proceedings of the 14th European Conference on Computer Vision (ECCV 2016)",
  pages = "382--398",
  address = "Cham, Switzerland",
  publisher = "Springer"
}

% Embedding-Based Metrics
@inproceedings{z:20,
  author = "Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q. and Artzi, Yoav",
  year = 2020,
  title = "{BERTScore: Evaluating Text Generation with BERT}",
  booktitle = "Proceedings of the Eighth International Conference on Learning Representations (ICLR 2020)",
  address = "Addis Ababa, Ethiopia",
  publisher = "OpenReview.net"
}

@inproceedings{z:19,
  author = "Zhao, Wei and Peyrard, Maxime and Liu, Fei and Gao, Yang and Meyer, Christian M. and Eger, Steffen",
  year = 2019,
  title = "{MoverScore: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance}",
  booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
  pages = "563--578",
  address = "Hong Kong, China",
  publisher = "Association for Computational Linguistics"
}

@inproceedings{r:19,
  author = "Reimers, Nils and Gurevych, Iryna",
  year = 2019,
  title = "{Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks}",
  booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
  pages = "3982--3992",
  address = "Hong Kong, China",
  publisher = "Association for Computational Linguistics"
}

@misc{l:24,
  author = "Lee, Chankyu and Roy, Rajarshi and Xu, Mengyao and Raiman, Jonathan and Shoeybi, Mohammad and Catanzaro, Bryan and Ping, Wei",
  year = 2024,
  title = "{NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models}",
  eprint = "2405.17428",
  archivePrefix= "arXiv",
  primaryClass = "cs.CL"
}

@misc{cklg:24,
  author = "Choi, Chanyeol and Kim, Junseong and Lee, Seolhwa and Kwon, Jihoon and Gu, Sangmo and Kim, Yejin and Cho, Minkyung and Sohn, Jy-yong",
  year = 2024,
  title = "{Linq-Embed-Mistral: Technical Report}",
  eprint = "2412.03223",
  archivePrefix= "arXiv",
  primaryClass = "cs.CL"
}

@misc{mljx:24,
  author = "Meng, Rui and Liu, Ye and Joty, Shafiq Rayhan and Xiong, Caiming and Zhou, Yingbo and Yavuz, Semih",
  year = 2024,
  title = "{SFR-Embedding-2: Advanced Text Embedding with Multi-stage Training}",
  howpublished = "\url{https://huggingface.co/Salesforce/SFR-Embedding-2_R}"
}

@misc{zlw:24,
  author = "Zhang, Dun and Li, Jiacheng and Zeng, Ziyang and Wang, Fulong",
  year = 2024,
  title = "{Jasper and Stella: distillation of SOTA embedding models}",
  eprint = "2412.19048",
  archivePrefix= "arXiv",
  primaryClass = "cs.IR"
}

% Multimodal Evaluation Metrics
@inproceedings{syxl:22,
  author = "Shi, Yaya and Yang, Xu and Xu, Haiyang and Yuan, Chunfeng and Li, Bing and Hu, Weiming and Zha, Zheng-Jun",
  year = 2022,
  title = "{EMScore: Evaluating Video Captioning via Coarse-Grained and Fine-Grained Embedding Matching}",
  booktitle = "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2022)",
  pages = "17929--17938",
  address = "Los Alamitos, Calif.",
  publisher = "IEEE Computer Society"
}

@inproceedings{sbc:23,
  author = "Sarto, Sara and Barraco, Manuele and Cornia, Marcella and Baraldi, Lorenzo and Cucchiara, Rita",
  year = 2023,
  title = "{Positive-Augmented Contrastive Learning for Image and Video Captioning Evaluation}",
  booktitle = "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2023)",
  pages = "6914--6924",
  address = "Los Alamitos, Calif.",
  publisher = "IEEE Computer Society"
}

@inproceedings{hflc:21,
  author = "Hessel, Jack and Holtzman, Ari and Forbes, Maxwell and Le Bras, Ronan and Choi, Yejin",
  year = 2021,
  title = "{CLIPScore: A Reference-free Evaluation Metric for Image Captioning}",
  booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
  pages = "7514--7528",
  address = "Punta Cana, Dominican Republic",
  publisher = "Association for Computational Linguistics"
}

@misc{wyzs:24,
  author = "Wang, Jiawei and Yuan, Liping and Zhang, Yuchen and Sun, Haomiao",
  year = 2024,
  title = "{Tarsier: Recipes for Training and Evaluating Large Video Description Models}",
  eprint = "2407.00634",
  archivePrefix= "arXiv",
  primaryClass = "cs.CV"
}

@inproceedings{dp:25,
  author = "Dubey, Harsh and Pack, Chulwoo",
  year = 2025,
  title = "{Leveraging Textual Memory and Key Frame Reasoning for Full Video Understanding Using Off-the-Shelf LLMs and VLMs (Student Abstract)}",
  booktitle = "Proceedings of the Thirty-Ninth AAAI Conference on Artificial Intelligence (AAAI-25)",
  pages = "12445--12446",
  address = "Menlo Park, Calif.",
  publisher = "AAAI Press"
}

% LLM-Based Metrics
@inproceedings{chan:23,
  author    = "Chan, David M. and Petryk, Suzanne and Gonzalez, Joseph E. and Darrell, Trevor and Canny, John",
  year      = 2023,
  title     = "{CLAIR: Evaluating Image Captions with Large Language Models}",
  booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing ({EMNLP}‑2023)",
  pages     = "13638--13646",
  address   = "Singapore",
  publisher = "Association for Computational Linguistics"
}

@misc{cheng:25,
  author       = "Cheng, Kanzhi and Song, Wenpo and Fan, Jiaxin and Ma, Zheng and Sun, Qiushi and Xu, Fangzhi and Yan, Chenyang and Chen, Nuo and Zhang, Jianbing and Chen, Jiajun",
  year         = 2025,
  title        = "{CapArena: Benchmarking and Analyzing Detailed Image Captioning in the LLM Era}",
  eprint       = "2503.12329",
  archivePrefix= "arXiv",
  primaryClass = "cs.CV",
  note         = "arXiv preprint"
}

@misc{li:24,
  author       = "Li, Boyi and Zhu, Ligeng and Tian, Ran and Tan, Shuhan and Chen, Yuxiao and Lu, Yao and Cui, Yin and Veer, Sushant and Ehrlich, Max and Philion, Jonah and Weng, Xinshuo and Xue, Fuzhao and Fan, Jim and Zhu, Yuke and Kautz, Jan and Tao, Andrew and Liu, Ming-Yu and Fidler, Sanja and Ivanovic, Boris and Darrell, Trevor and Malik, Jitendra and Han, Song and Pavone, Marco",
  year         = 2024,
  title        = "{Wolf: Dense Video Captioning with a World Summarization Framework}",
  eprint       = "2407.18908",
  archivePrefix= "arXiv",
  primaryClass = "cs.CV",
  note         = "arXiv preprint"
}

@misc{rawal:25,
  author       = "Rawal, Ruchit and Shirkavand, Reza and Huang, Heng and Somepalli, Gowthami and Goldstein, Tom",
  year         = 2025,
  title        = "{ARGUS: Hallucination and Omission Evaluation in Video-LLMs}",
  eprint       = "2506.07371",
  archivePrefix= "arXiv",
  primaryClass = "cs.CV",
  note         = "arXiv preprint"
}